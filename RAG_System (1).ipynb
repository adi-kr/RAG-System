{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WULI49ofFOe2"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pdfplumber transformers sentence-transformers pytesseract faiss-cpu opencv-python-headless Pillow\n",
        "\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import util\n",
        "\n",
        "# Initialize necessary models\n",
        "nlp_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "cross_encoder_model = BertForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "cross_encoder_tokenizer = BertTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "index = faiss.IndexFlatL2(384)  # FAISS index for vector search\n",
        "\n",
        "# Mock chat memory (to replace Redis)\n",
        "chat_memory = {}\n",
        "\n",
        "def store_chat_memory(session_id, question, answer):\n",
        "    chat_memory[session_id] = {\"question\": question, \"answer\": answer}\n",
        "\n",
        "def retrieve_chat_memory(session_id):\n",
        "    return chat_memory.get(session_id)\n",
        "\n",
        "# 1. PDF Text Extraction using pdfplumber\n",
        "def extract_text_from_pdf(pdf_path, is_scanned=False, lang='eng'):\n",
        "    if not is_scanned:\n",
        "        # Use pdfplumber for extracting text from digital PDFs\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    else:\n",
        "        # OCR extraction for scanned PDFs\n",
        "        img = cv2.imread(pdf_path)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        return pytesseract.image_to_string(gray, lang=lang)\n",
        "\n",
        "# 2. Query decomposition\n",
        "def decompose_query(query):\n",
        "    sub_queries = query.split(' and ')  # Simple example, can be enhanced with NLP models\n",
        "    return sub_queries\n",
        "\n",
        "# 3. Optimized chunking for large documents\n",
        "def chunk_text(text, max_tokens=200):\n",
        "    chunks = []\n",
        "    chunk = \"\"\n",
        "    for sentence in text.split('. '):\n",
        "        if len(chunk.split()) + len(sentence.split()) < max_tokens:\n",
        "            chunk += sentence + \". \"\n",
        "        else:\n",
        "            chunks.append(chunk.strip())\n",
        "            chunk = sentence + \". \"\n",
        "    chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def hybrid_search(query):\n",
        "\n",
        "    keyword_results = [{'text': 'This is a mocked keyword search result.'}]\n",
        "\n",
        "    # Semantic Search using SentenceTransformer\n",
        "    query_embedding = nlp_model.encode(query)\n",
        "    sentence_embeddings = nlp_model.encode([result['text'] for result in keyword_results])\n",
        "    semantic_results = util.semantic_search(query_embedding, sentence_embeddings)\n",
        "\n",
        "    return keyword_results, semantic_results\n",
        "\n",
        "# 5. FAISS vector search\n",
        "def add_to_faiss_index(embeddings):\n",
        "    index.add(embeddings)\n",
        "\n",
        "def search_faiss(query_embedding, top_k=10):\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "    return distances, indices\n",
        "\n",
        "# 6. Reranking using Cross-Encoder\n",
        "def rerank(query, documents):\n",
        "    reranked_results = []\n",
        "    for doc in documents:\n",
        "        inputs = cross_encoder_tokenizer(query, doc, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        scores = cross_encoder_model(**inputs).logits\n",
        "        reranked_results.append((doc, scores.item()))\n",
        "\n",
        "    reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return reranked_results\n",
        "\n",
        "# RAG pipeline\n",
        "def rag_pipeline(pdf_path, is_scanned=False, lang='eng', session_id=None, query=None):\n",
        "    # 1. Extract text\n",
        "    text = extract_text_from_pdf(pdf_path, is_scanned, lang)\n",
        "\n",
        "    # 2. Store extracted text in chunks\n",
        "    chunks = chunk_text(text)\n",
        "\n",
        "    # 3. Add chunk embeddings to FAISS\n",
        "    embeddings = nlp_model.encode(chunks)\n",
        "    add_to_faiss_index(embeddings)\n",
        "\n",
        "    # 4. Perform Hybrid Search (Keyword + Semantic)\n",
        "    keyword_results, semantic_results = hybrid_search(query)\n",
        "\n",
        "    # 5. Perform FAISS search for the query embedding\n",
        "    query_embedding = nlp_model.encode(query)\n",
        "    distances, indices = search_faiss(query_embedding)\n",
        "\n",
        "    # 6. Rerank the results using the cross-encoder model\n",
        "    reranked = rerank(query, chunks)\n",
        "\n",
        "    # 7. Store the conversation context in chat memory\n",
        "    if session_id:\n",
        "        store_chat_memory(session_id, query, reranked[0][0])\n",
        "\n",
        "    return {\n",
        "        \"keyword_results\": keyword_results,\n",
        "        \"semantic_results\": semantic_results,\n",
        "        \"faiss_results\": indices,\n",
        "        \"reranked_results\": reranked\n",
        "    }\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZSRXQxKEFQgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "id": "qlB_y7f3Fun_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = next(iter(uploaded.keys()))  # Get the first uploaded file\n",
        "session_id = \"1\"\n",
        "query = \"who is Jerry?\"\n",
        "\n",
        "# Run the RAG pipeline\n",
        "results = rag_pipeline(pdf_path, is_scanned=False, lang='eng', session_id=session_id, query=query)\n",
        "\n",
        "# Display the results\n",
        "print(results)"
      ],
      "metadata": {
        "id": "60SKKM5JHh1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SLj261MJgwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}